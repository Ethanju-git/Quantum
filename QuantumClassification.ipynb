{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:38:23.109787Z",
     "iopub.status.busy": "2025-04-06T04:38:23.109389Z",
     "iopub.status.idle": "2025-04-06T04:38:25.922956Z",
     "shell.execute_reply": "2025-04-06T04:38:25.921695Z",
     "shell.execute_reply.started": "2025-04-06T04:38:23.109754Z"
    }
   },
   "outputs": [],
   "source": [
    "!python3 -m venv qml-env\n",
    "!source qml-env/bin/activate  # on Windows: .\\qml-env\\Scripts\\activate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:38:25.924965Z",
     "iopub.status.busy": "2025-04-06T04:38:25.924660Z",
     "iopub.status.idle": "2025-04-06T04:38:33.488263Z",
     "shell.execute_reply": "2025-04-06T04:38:33.487255Z",
     "shell.execute_reply.started": "2025-04-06T04:38:25.924925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pennylane\n",
      "  Downloading PennyLane-0.40.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy<2.1 in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.26.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.11.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pennylane) (3.2.1)\n",
      "Collecting rustworkx>=0.14.0 (from pennylane)\n",
      "  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting autograd (from pennylane)\n",
      "  Downloading autograd-1.7.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting tomlkit (from pennylane)\n",
      "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: appdirs in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.4.4)\n",
      "Collecting autoray>=0.6.11 (from pennylane)\n",
      "  Downloading autoray-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from pennylane) (5.3.2)\n",
      "Collecting pennylane-lightning>=0.40 (from pennylane)\n",
      "  Downloading PennyLane_Lightning-0.40.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pennylane) (4.9.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pennylane) (23.2)\n",
      "Collecting diastatic-malt (from pennylane)\n",
      "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.40->pennylane)\n",
      "  Downloading scipy_openblas32-0.3.29.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
      "Requirement already satisfied: gast in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (0.5.4)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (2.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->pennylane) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->pennylane) (2020.6.20)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (0.35.1)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/lib/python3/dist-packages (from astunparse->diastatic-malt->pennylane) (1.16.0)\n",
      "Downloading PennyLane-0.40.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading autoray-0.7.1-py3-none-any.whl (930 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PennyLane_Lightning-0.40.0-cp311-cp311-manylinux_2_28_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading autograd-1.7.0-py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading scipy_openblas32-0.3.29.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tomlkit, scipy-openblas32, rustworkx, autoray, autograd, diastatic-malt, pennylane-lightning, pennylane\n",
      "Successfully installed autograd-1.7.0 autoray-0.7.1 diastatic-malt-2.15.2 pennylane-0.40.0 pennylane-lightning-0.40.0 rustworkx-0.16.0 scipy-openblas32-0.3.29.0.0 tomlkit-0.13.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pennylane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:14:35.792634Z",
     "iopub.status.busy": "2025-04-06T04:14:35.792307Z",
     "iopub.status.idle": "2025-04-06T04:14:39.027791Z",
     "shell.execute_reply": "2025-04-06T04:14:39.026101Z",
     "shell.execute_reply.started": "2025-04-06T04:14:35.792610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: PennyLane_Lightning\n",
      "Version: 0.40.0\n",
      "Summary: PennyLane-Lightning plugin\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: Apache License 2.0\n",
      "Location: /usr/local/lib/python3.11/dist-packages\n",
      "Requires: pennylane, scipy-openblas32\n",
      "Required-by: PennyLane\n"
     ]
    }
   ],
   "source": [
    "!pip show pennylane-lightning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:38:33.490879Z",
     "iopub.status.busy": "2025-04-06T04:38:33.490510Z",
     "iopub.status.idle": "2025-04-06T04:38:39.903133Z",
     "shell.execute_reply": "2025-04-06T04:38:39.902105Z",
     "shell.execute_reply.started": "2025-04-06T04:38:33.490840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (23.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.21)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.66.1)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from optuna) (5.4.1)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.12 (from alembic>=1.5.0->optuna)\n",
      "  Downloading typing_extensions-4.13.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/lib/python3/dist-packages (from sqlalchemy>=1.4.2->optuna) (1.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.4)\n",
      "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading typing_extensions-4.13.1-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: typing-extensions, Mako, colorlog, alembic, optuna\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "Successfully installed Mako-1.3.9 alembic-1.15.2 colorlog-6.9.0 optuna-4.2.1 typing-extensions-4.13.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:38:39.904749Z",
     "iopub.status.busy": "2025-04-06T04:38:39.904392Z",
     "iopub.status.idle": "2025-04-06T04:38:39.914844Z",
     "shell.execute_reply": "2025-04-06T04:38:39.913134Z",
     "shell.execute_reply.started": "2025-04-06T04:38:39.904709Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1131985662.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install tensorflow==2.12\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.12\n",
    "pip install tensorflow-quantum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T05:48:20.349623Z",
     "iopub.status.busy": "2025-04-06T05:48:20.349136Z",
     "iopub.status.idle": "2025-04-06T06:32:17.913339Z",
     "shell.execute_reply": "2025-04-06T06:32:17.912422Z",
     "shell.execute_reply.started": "2025-04-06T05:48:20.349582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n",
      "[I 2025-04-06 05:48:22,112] A new study created in memory with name: no-name-6f12620b-ff56-4117-aaed-7450a930a5c7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples used (10% subset): 4884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42/313193639.py:84: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-4, 1e-1)\n",
      "/tmp/ipykernel_42/313193639.py:85: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n",
      "[I 2025-04-06 05:48:24,016] Trial 0 finished with value: 0.6140192747116089 and parameters: {'lr': 0.0006968350728123136, 'weight_decay': 1.7389225710840467e-06, 'n_epochs': 41}. Best is trial 0 with value: 0.6140192747116089.\n",
      "[I 2025-04-06 05:48:25,462] Trial 1 finished with value: 0.5134274959564209 and parameters: {'lr': 0.060892150727319294, 'weight_decay': 0.0005753744964653768, 'n_epochs': 27}. Best is trial 1 with value: 0.5134274959564209.\n",
      "[I 2025-04-06 05:48:27,503] Trial 2 finished with value: 0.5411403179168701 and parameters: {'lr': 0.001411355170199002, 'weight_decay': 0.0034961518895528796, 'n_epochs': 46}. Best is trial 1 with value: 0.5134274959564209.\n",
      "[I 2025-04-06 05:48:29,174] Trial 3 finished with value: 0.5076606869697571 and parameters: {'lr': 0.06217367624213928, 'weight_decay': 1.30896899505966e-06, 'n_epochs': 40}. Best is trial 3 with value: 0.5076606869697571.\n",
      "[I 2025-04-06 05:48:30,148] Trial 4 finished with value: 0.5463396310806274 and parameters: {'lr': 0.005500415158024001, 'weight_decay': 0.004067731806388384, 'n_epochs': 17}. Best is trial 3 with value: 0.5076606869697571.\n",
      "[I 2025-04-06 05:48:32,198] Trial 5 finished with value: 0.5347549915313721 and parameters: {'lr': 0.001773006728023926, 'weight_decay': 0.0001229220525415166, 'n_epochs': 48}. Best is trial 3 with value: 0.5076606869697571.\n",
      "[I 2025-04-06 05:48:34,078] Trial 6 finished with value: 0.9039703607559204 and parameters: {'lr': 0.00010107998673103309, 'weight_decay': 0.004688892988651916, 'n_epochs': 39}. Best is trial 3 with value: 0.5076606869697571.\n",
      "[I 2025-04-06 05:48:35,237] Trial 7 finished with value: 0.5250234007835388 and parameters: {'lr': 0.046251244296693084, 'weight_decay': 0.006282880322620595, 'n_epochs': 23}. Best is trial 3 with value: 0.5076606869697571.\n",
      "[I 2025-04-06 05:48:36,820] Trial 8 finished with value: 0.542502760887146 and parameters: {'lr': 0.0009878471496875163, 'weight_decay': 0.002132326734974211, 'n_epochs': 35}. Best is trial 3 with value: 0.5076606869697571.\n",
      "[I 2025-04-06 05:48:38,641] Trial 9 finished with value: 0.5210701823234558 and parameters: {'lr': 0.018829350230668514, 'weight_decay': 2.012239286677394e-05, 'n_epochs': 37}. Best is trial 3 with value: 0.5076606869697571.\n",
      "[I 2025-04-06 05:48:38,941] Trial 10 finished with value: 0.5291586518287659 and parameters: {'lr': 0.011238674947664411, 'weight_decay': 1.241897203761186e-06, 'n_epochs': 8}. Best is trial 3 with value: 0.5076606869697571.\n",
      "[I 2025-04-06 05:48:40,369] Trial 11 finished with value: 0.511547327041626 and parameters: {'lr': 0.09755699354109776, 'weight_decay': 0.0002609882678418766, 'n_epochs': 27}. Best is trial 3 with value: 0.5076606869697571.\n",
      "[I 2025-04-06 05:48:41,879] Trial 12 finished with value: 0.5183725357055664 and parameters: {'lr': 0.0764770614951601, 'weight_decay': 1.3493655774555319e-05, 'n_epochs': 31}. Best is trial 3 with value: 0.5076606869697571.\n",
      "[I 2025-04-06 05:48:42,621] Trial 13 finished with value: 0.5346453189849854 and parameters: {'lr': 0.019335602494298398, 'weight_decay': 0.00020994151687626843, 'n_epochs': 15}. Best is trial 3 with value: 0.5076606869697571.\n",
      "[I 2025-04-06 05:48:43,655] Trial 14 finished with value: 0.5136770009994507 and parameters: {'lr': 0.0886695254652845, 'weight_decay': 2.2720069859076473e-05, 'n_epochs': 24}. Best is trial 3 with value: 0.5076606869697571.\n",
      "[I 2025-04-06 05:48:44,870] Trial 15 finished with value: 0.5156322717666626 and parameters: {'lr': 0.02800750634381399, 'weight_decay': 0.000611260579777066, 'n_epochs': 30}. Best is trial 3 with value: 0.5076606869697571.\n",
      "[I 2025-04-06 05:48:46,624] Trial 16 finished with value: 0.5214892029762268 and parameters: {'lr': 0.006151357614945493, 'weight_decay': 5.438813757004228e-06, 'n_epochs': 43}. Best is trial 3 with value: 0.5076606869697571.\n",
      "[I 2025-04-06 05:48:49,127] Trial 17 finished with value: 0.5372165441513062 and parameters: {'lr': 0.00022225973045988855, 'weight_decay': 5.0278026600467775e-05, 'n_epochs': 50}. Best is trial 3 with value: 0.5076606869697571.\n",
      "[I 2025-04-06 05:48:50,105] Trial 18 finished with value: 0.5307052135467529 and parameters: {'lr': 0.03158350782792248, 'weight_decay': 0.0004228537763072944, 'n_epochs': 19}. Best is trial 3 with value: 0.5076606869697571.\n",
      "[I 2025-04-06 05:48:51,650] Trial 19 finished with value: 0.5376147627830505 and parameters: {'lr': 0.010617796294431991, 'weight_decay': 3.887556985752212e-06, 'n_epochs': 34}. Best is trial 3 with value: 0.5076606869697571.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters from surrogate tuning: {'lr': 0.06217367624213928, 'weight_decay': 1.30896899505966e-06, 'n_epochs': 40}\n",
      "Test Accuracy: 0.7778915166854858\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Use GPU if available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# -----------------------\n",
    "# 1. DATASET PREPARATION\n",
    "# -----------------------\n",
    "\n",
    "# Load the Adult dataset from OpenML.\n",
    "data = fetch_openml(\"adult\", version=2, as_frame=True)\n",
    "df = data.frame\n",
    "\n",
    "# Select only numeric features.\n",
    "numeric_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "X = df[numeric_cols].to_numpy()\n",
    "y = df['class'].to_numpy()\n",
    "# Convert target to binary: 0 for \"<=50K\", 1 for \">50K\".\n",
    "y = np.where(y == '<=50K', 0, 1)\n",
    "\n",
    "# For demonstration, use only 10% of the samples.\n",
    "np.random.seed(42)\n",
    "indices = np.random.choice(X.shape[0], size=int(0.1 * X.shape[0]), replace=False)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "print(\"Number of samples used (10% subset):\", X.shape[0])\n",
    "\n",
    "# Reduce features to 2 using PCA (for our 2-qubit circuit).\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Scale features to [-π, π].\n",
    "scaler = MinMaxScaler(feature_range=(-np.pi, np.pi))\n",
    "X_scaled = scaler.fit_transform(X_pca)\n",
    "\n",
    "# Split into train+validation (80%) and test (20%),\n",
    "# then further split train+validation into training (60%) and validation (20%).\n",
    "X_train_val_np, X_test_np, y_train_val_np, y_test_np = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "X_train_np, X_val_np, y_train_np, y_val_np = train_test_split(\n",
    "    X_train_val_np, y_train_val_np, test_size=0.25, random_state=42, stratify=y_train_val_np\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors and move to device.\n",
    "X_train = torch.tensor(X_train_np, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "X_val = torch.tensor(X_val_np, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val_np, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "X_test = torch.tensor(X_test_np, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test_np, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "# Optionally, create a DataLoader for tuning.\n",
    "batch_size = 256\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. CLASSICAL SURROGATE MODEL FOR TUNING\n",
    "# ----------------------------------------\n",
    "\n",
    "# We define a fast classical model (a simple linear model) to tune hyperparameters.\n",
    "class ClassicalSurrogate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1)  # directly mapping from the 2 PCA features to a logit\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters.\n",
    "    lr = trial.suggest_loguniform('lr', 1e-4, 1e-1)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n",
    "    n_epochs = trial.suggest_int('n_epochs', 5, 50)  # keep epochs small for fast tuning\n",
    "    \n",
    "    model = ClassicalSurrogate().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val)\n",
    "        val_loss = criterion(val_outputs, y_val).item()\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters from surrogate tuning:\", study.best_trial.params)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. QUANTUM CIRCUIT DEFINITION\n",
    "# ----------------------------------------\n",
    "\n",
    "# Optionally, check for PennyLane-Lightning:\n",
    "# Run `pip show pennylane-lightning` in your terminal.\n",
    "# If not installed, you can install via: pip install pennylane-lightning\n",
    "# Here we switch to the Lightning simulator for potentially faster simulation:\n",
    "dev = qml.device(\"lightning.qubit\", wires=2)\n",
    "observable = qml.Hamiltonian([1, 1], [qml.PauliZ(0), qml.PauliZ(1)])\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_layer_q(inputs, weights):\n",
    "    if inputs.ndim == 2:\n",
    "        inputs = inputs[0]\n",
    "    qml.RX(inputs[0], wires=0)\n",
    "    qml.RX(inputs[1], wires=1)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.RY(weights[0], wires=0)\n",
    "    qml.RY(weights[1], wires=1)\n",
    "    exp_val = qml.expval(observable)\n",
    "    return qml.math.reshape(exp_val, (1,))\n",
    "\n",
    "weight_shapes = {\"weights\": (2,)}\n",
    "qlayer = qml.qnn.TorchLayer(quantum_layer_q, weight_shapes=weight_shapes)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. HYBRID QUANTUM-CLASSICAL MODEL\n",
    "# ----------------------------------------\n",
    "\n",
    "class HybridQNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quantum = qlayer  # Expects input shape (1,2) and returns (1,)\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        q_out_list = []\n",
    "        for sample in inputs:\n",
    "            sample_batched = sample.unsqueeze(0)  # (1,2)\n",
    "            q_out_sample = self.quantum(sample_batched)  # (1,)\n",
    "            q_out_list.append(q_out_sample.squeeze(0))   # (1,)\n",
    "        q_out = torch.stack(q_out_list)\n",
    "        if q_out.ndim == 1:\n",
    "            q_out = q_out.unsqueeze(1)\n",
    "        out = self.linear(q_out)\n",
    "        return out\n",
    "\n",
    "# ----------------------------------------\n",
    "# 5. RETRAIN FINAL MODEL & EVALUATION\n",
    "# ----------------------------------------\n",
    "\n",
    "# Combine training and validation sets.\n",
    "X_train_full = torch.cat((X_train, X_val), dim=0)\n",
    "y_train_full = torch.cat((y_train, y_val), dim=0)\n",
    "\n",
    "best_params = study.best_trial.params\n",
    "final_model = HybridQNN().to(device)\n",
    "final_optimizer = optim.Adam(final_model.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "n_epochs_final = best_params['n_epochs']\n",
    "final_dataset = TensorDataset(X_train_full, y_train_full)\n",
    "final_loader = DataLoader(final_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(n_epochs_final):\n",
    "    final_model.train()\n",
    "    for X_batch, y_batch in final_loader:\n",
    "        final_optimizer.zero_grad()\n",
    "        outputs = final_model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        final_optimizer.step()\n",
    "\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_logits = final_model(X_test)\n",
    "    test_preds = (torch.sigmoid(test_logits) > 0.5).float()\n",
    "    accuracy = (test_preds.eq(y_test).sum() / y_test.shape[0]).item()\n",
    "    print(\"Test Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
